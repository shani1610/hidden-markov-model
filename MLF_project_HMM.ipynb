{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4je-ekC1l6iU"
      },
      "source": [
        "# Machine Learning Fundamentals Project\n",
        "## Sentiment Analysis with Hidden Markov Model\n",
        "\n",
        "Here is an overview of everyone who contributed to the project:\n",
        "\n",
        "|First name|Last name|Master program|Contribution|\n",
        "|----------|---------|--------------|-------------|\n",
        "|Anna Lena Katharina|Braun|IMLEX|25%|\n",
        "|Aryaman|Sharma|IMLEX|25%|\n",
        "|Raffael|Rizzo|IMLEX|25%|\n",
        "|Shani|Israelov|IMLEX|25%|"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "collapsed": false,
        "id": "zjSDdmYcfL2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tyj9Ac1Wlz1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "334c1536-a8f8-4f14-c9b7-1286dc7a82e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hmmlearn==0.2.6 in /usr/local/lib/python3.9/dist-packages (0.2.6)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.9/dist-packages (from hmmlearn==0.2.6) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.9/dist-packages (from hmmlearn==0.2.6) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.9/dist-packages (from hmmlearn==0.2.6) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.6) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.6) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hmmlearn==0.2.6\n",
        "from hmmlearn import hmm\n",
        "\n",
        "import numpy as np  # linear algebra\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt  # show graph\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
        "    f1_score, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaYld-8EiAyH",
        "outputId": "97777864-c018-4864-f2e4-712249b50260"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEiMw2lwmhy1"
      },
      "source": [
        "In this notebook we will look at the NER dataset and use it to understand HMM and also construct a POS tagger at the same time.\n",
        "\n",
        "# Load data\n",
        "\n",
        "## Data Description \n",
        "\n",
        "Imagine a dense jungle of words, where language constructs form dense thickets and\n",
        "sentences rise like towering trees. Named Entity Recognition is the art of slicing\n",
        "through these thickets, identifying and categorizing the entities that hide within. For\n",
        "our group project, we will use the Hidden Markov Model to perform Named Entity\n",
        "Recognition.\n",
        "\n",
        "\n",
        "Kaggle hosts a popular dataset for this objective, with the convenient\n",
        "name of Named Entity Recognition (NER). It can be accessed under this link: https://www.kaggle.com/datasets/debasisdotcom/name-entity-recognition-ner-dataset. \n",
        "\n",
        "The NER dataset consists of a collection of annotated sentences designed to\n",
        "train and evaluate machine learning models for the task of named entity recognition.\n",
        "It contains labeled sentences from various sources, like news articles, and spans mul-\n",
        "tiple domains auch as sports, politics, and entertainment. The goal of an NER model\n",
        "trained on this dataset is to learn how to identify and classify named entities within\n",
        "a given text.\n",
        "\n",
        "\n",
        "The NER dataset consists of four columns:\n",
        "\n",
        "\n",
        "* **Sentence (or ID):** This column contains\n",
        "a unique identifier for each sentence in\n",
        "the dataset. The identifies distinguishes and references individual sentences in the\n",
        "NER dataset.\n",
        "* **Word:** This column contains individual words, also called tokens, which form the\n",
        "sentence. Hence, each row represents a single word from the sentence.\n",
        "* **POS (Part of Speech):** This column contains the part of speech tag for the\n",
        "corresponding word.\n",
        "* **Tag:** This column represents the named entity tag associated with the word. The\n",
        "tags are usually in the IOB format (Inside, Outside, Beginning) and are categorized\n",
        "into different classes like B-PER (beginning of a person’s name), I-PER (inside a\n",
        "person’s name), B-ORG (beginning of an organization name), I-ORG (inside an\n",
        "organization name), B-LOC (beginning of a location name), I-LOC (inside a location\n",
        "name), and O (outside any named entity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P8PDOUWFsoiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900a18db-878b-4543-d6b3-4a2460c1cce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/NER dataset.csv\", encoding='latin1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.fillna(method=\"ffill\")\n",
        "data = data.rename(columns={'Sentence #': 'sentence'})\n",
        "data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gQKsIZYZmkOT",
        "outputId": "12953f2c-fdc6-4e53-a061-bd6ebda7ec1a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      sentence           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1  Sentence: 1             of   IN   O\n",
              "2  Sentence: 1  demonstrators  NNS   O\n",
              "3  Sentence: 1           have  VBP   O\n",
              "4  Sentence: 1        marched  VBN   O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e12beaa0-cb2b-49d2-acb8-acfbe340ee97\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e12beaa0-cb2b-49d2-acb8-acfbe340ee97')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e12beaa0-cb2b-49d2-acb8-acfbe340ee97 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e12beaa0-cb2b-49d2-acb8-acfbe340ee97');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bieothyVtLxy"
      },
      "source": [
        "# Data pre-processing\n",
        "If you want to do some pre-processing (lowercase any words, remove stop words, replace numbers/names by a unique NUM/NAME token, etc.) you can do it here in the pipeline.\n",
        "\n",
        "Note : you could create a new dataset `data_pre_precessed = pre_process(data)` to keep both version and compare the effect of you pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "txb_5PPstsKc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8650b9e1-9cd9-44e3-8561-7115e27b029c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      sentence           Word  POS Tag\n",
              "0  sentence: 1      thousands  NNS   O\n",
              "1  sentence: 1             of   IN   O\n",
              "2  sentence: 1  demonstrators  NNS   O\n",
              "3  sentence: 1           have  VBP   O\n",
              "4  sentence: 1        marched  VBN   O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39d69b09-5d73-4a87-b270-c8dc6a221eba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39d69b09-5d73-4a87-b270-c8dc6a221eba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39d69b09-5d73-4a87-b270-c8dc6a221eba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39d69b09-5d73-4a87-b270-c8dc6a221eba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "def pre_processing(df):\n",
        "    # Select the columns to apply lowercase transformation\n",
        "    columns_to_lowercase = ['sentence', 'Word']\n",
        "    \n",
        "    # Apply lowercase transformation to the selected columns\n",
        "    df[columns_to_lowercase] = df[columns_to_lowercase].applymap(str.lower)\n",
        "    \n",
        "    # Return the modified DataFrame\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/NER dataset.csv\", encoding='latin1')\n",
        "data = data.fillna(method=\"ffill\")\n",
        "data = data.rename(columns={'Sentence #': 'sentence'})\n",
        "\n",
        "# Apply pre-processing to the DataFrame\n",
        "data = pre_processing(data)\n",
        "data.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIPM3ZblnIfB"
      },
      "source": [
        "First let's collect the unique words and the unique POS tags in the dataset, we will use this to construct the HMM later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8ztujKumbQb",
        "outputId": "b1b20564-2e5f-4074-f7c8-c354c54f2fe8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 31817)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
        "words = list(set(data.Word.values))  # Unique words in the dataset\n",
        "len(tags), len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXihLYTNnV84"
      },
      "source": [
        "### We have 42 different tags and 35,178 different words, so the HMM that we construct will have the following properties\n",
        "- The hidden states of the this HMM will correspond to the POS tags, so we will have 42 hidden states.\n",
        "- The Observations for this HMM will correspond to the sentences and their words.\n",
        "\n",
        "#### Before constructing the HMM, we will split the data into train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3Xb_rvZ6nQNH"
      },
      "outputs": [],
      "source": [
        "y = data.POS\n",
        "X = data.drop('POS', axis=1)\n",
        "\n",
        "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
        "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
        "\n",
        "data_train = data.loc[train_ix]\n",
        "data_test = data.loc[test_ix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FhZFM-48t3OI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cc3dfe1e-6027-4e1a-9cb3-24d88b2f7c49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       sentence      Word  POS Tag\n",
              "24  sentence: 2  families  NNS   O\n",
              "25  sentence: 2        of   IN   O\n",
              "26  sentence: 2  soldiers  NNS   O\n",
              "27  sentence: 2    killed  VBN   O\n",
              "28  sentence: 2        in   IN   O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a58f8826-ca95-4d97-8eff-e28cab86eb7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>families</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>soldiers</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>killed</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>sentence: 2</td>\n",
              "      <td>in</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a58f8826-ca95-4d97-8eff-e28cab86eb7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a58f8826-ca95-4d97-8eff-e28cab86eb7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a58f8826-ca95-4d97-8eff-e28cab86eb7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "data_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FMrZCuzut6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ffea13fa-100c-4a21-8ebf-4a40c345ba4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      sentence           Word  POS Tag\n",
              "0  sentence: 1      thousands  NNS   O\n",
              "1  sentence: 1             of   IN   O\n",
              "2  sentence: 1  demonstrators  NNS   O\n",
              "3  sentence: 1           have  VBP   O\n",
              "4  sentence: 1        marched  VBN   O"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b982ec6-c377-4676-97f5-f77d996081a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sentence: 1</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b982ec6-c377-4676-97f5-f77d996081a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b982ec6-c377-4676-97f5-f77d996081a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b982ec6-c377-4676-97f5-f77d996081a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data_test.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu1fwHIzo0KU"
      },
      "source": [
        "Now lets encode the POS and Words to be used to generate the HMM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj8cnwcOoznK",
        "outputId": "56dc0e30-7219-4abc-e41d-5a4b82f56d03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 25100)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
        "dfupdate.Word = 'UNKNOWN'\n",
        "data_train.update(dfupdate)\n",
        "words = list(set(data_train.Word.values))\n",
        "# Convert words and tags into numbers\n",
        "word2id = {w: i for i, w in enumerate(words)}\n",
        "tag2id = {t: i for i, t in enumerate(tags)}\n",
        "id2tag = {i: t for i, t in enumerate(tags)}\n",
        "len(tags), len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koiN38BSpNZb"
      },
      "source": [
        "In your theory classes you might have seen that the Hidden Markov Models can be learned by using the Baum-Welch algorithm by just using the observations.\n",
        "Although we can learn the Hidden States (POS tags) using Baum-Welch algorithm,We cannot map them back the states (words) to the POS tag. So for this exercise we will skip using the BW algorithm and directly create the HMM.\n",
        "\n",
        "For creating the HMM we should build the following three parameters. \n",
        "- `startprob_`\n",
        "- `transmat_`\n",
        "- `emissionprob_`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RXWyEXlqD0B"
      },
      "source": [
        "To construct the above mentioned paramters let's first create some useful matrices that will assist us in creating the above three parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uiXtl641o76N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1457b9-48b5-41d7-b501-31e97568ae04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 702936/702936 [00:00<00:00, 878326.88it/s]\n"
          ]
        }
      ],
      "source": [
        "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
        "# Now let's create the tags to words count\n",
        "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
        "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
        "# We shall also collect the counts for the first tags in the sentence\n",
        "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
        "\n",
        "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
        "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
        "sentences = list(data_train.sentence)\n",
        "pos = list(data_train.POS)\n",
        "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
        "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
        "        prevtagid = tag2id[pos[i - 1]]\n",
        "        nexttagid = tag2id[pos[i]]\n",
        "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S67MQmn5sCWJ"
      },
      "source": [
        "Now Let's build the parameter matrices "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "LPV6pioBqaey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9aa4848-bea2-415b-88b2-aa59f35efbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42/42 [00:00<00:00, 64.31it/s]\n"
          ]
        }
      ],
      "source": [
        "startprob = np.zeros((len(tags),))\n",
        "transmat = np.zeros((len(tags), len(tags)))\n",
        "emissionprob = np.zeros((len(tags), len(words)))\n",
        "num_sentences = sum(count_init_tags.values())\n",
        "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
        "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
        "    floatCountTag = float(count_tags.get(tag, 0))\n",
        "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
        "    for word, wordid in word2id.items():\n",
        "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
        "    for tag2, tagid2 in tag2id.items():\n",
        "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbca__ogusIu"
      },
      "source": [
        "# Task 1: \n",
        "\n",
        "Similar to how we built the hidden state transition probability matrix as shown above, you will built the transition probability between the words. With this matrix write a function that can calculate the log likelihood given a sentence.\n",
        "\n",
        "# Our result:\n",
        "\n",
        "In the code snippet underneath, we first initialize the word transition count matrix, which is a matrix of zeros with the shape (len(words), len(words)). This matrix represents the counts of transitions from one word to another in the training data. We then iterate through the words in the training data (data_train.Word) and update the count_words_to_next_words matrix. If two consecutive words belong to the same sentence, the count of the transition from the first word to the second word is incremented in the count_words_to_next_words matrix.\n",
        "\n",
        "After that, we normalize the counts in the count_words_to_next_words matrix by dividing each row by its sum, creating a probability matrix. Each element in this matrix, which we call the word_transition_matrix, represents the probability of transitioning from one word to another.\n",
        "\n",
        "We then implement the calculate_log_likelihood function that takes a sentence (as a list of words) and the word_transition_matrix as input. This function calculates the log-likelihood of the given sentence based on the word transition probabilities. The log-likelihood is the sum of the log probabilities of all transitions between consecutive words in the sentence. If a transition has a probability of zero, the log-likelihood is set to -np.inf.\n",
        "\n",
        "Finally, we calculate the log likelihood of a sample sentence and the given sentences using the calculate_log_likelihood function. We split the words in the sentences and pass them to the function along with the word_transition_matrix. The resulting log likelihood for each sentence is then printed.\n",
        "\n",
        "These steps are important for calculating the log likelihood of sentences based on word transition probabilities. By creating a matrix of word transition probabilities, we can analyze how likely it is for one word to follow another in a given sentence. The log likelihood can be used to compare different sentences or to evaluate the plausibility of a given sentence based on the training data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# Step 1: Initialize the word transition count matrix\n",
        "count_words_to_next_words = np.zeros((len(words), len(words)), dtype=int)\n",
        "\n",
        "# Step 2: Iterate through the training data and update the count_words_to_next_words matrix\n",
        "words_list = list(data_train.Word)\n",
        "for i in range(1, len(words_list)):\n",
        "    if sentences[i] == sentences[i - 1]:\n",
        "        prev_word_id = word2id[words_list[i - 1]]\n",
        "        next_word_id = word2id[words_list[i]]\n",
        "        count_words_to_next_words[prev_word_id][next_word_id] += 1\n",
        "\n",
        "# Step 3: Normalize the counts to create the word_transition_matrix\n",
        "word_transition_matrix = count_words_to_next_words / np.sum(count_words_to_next_words, axis=1, keepdims=True)\n",
        "\n",
        "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
        "    log_likelihood = 0\n",
        "    for i in range(1, len(sentence)):\n",
        "        prev_word_id = word2id[sentence[i - 1].lower()]\n",
        "        next_word_id = word2id[sentence[i].lower()]\n",
        "        prob = word_transition_matrix[prev_word_id][next_word_id]\n",
        "        log_likelihood += np.log(prob) if prob > 0 else -np.inf\n",
        "    return log_likelihood\n",
        "\n",
        "# Example usage\n",
        "log_likelihood = calculate_log_likelihood([\"This\", \"is\", \"a\", \"test\", \"sentence\"], word_transition_matrix)\n",
        "print(log_likelihood)\n",
        "\n",
        "sentences = [\n",
        "    \"This is a protest about how the new law is not in the interest of the people\",\n",
        "    \"The international conference will continue as planned on Friday\",\n",
        "    \"Who are you ?\",\n",
        "    \"You are not me\",\n",
        "    \"Do you expect to be happy to work late\"\n",
        "]\n",
        "\n",
        "for sentence in sentences:\n",
        "    words_list = sentence.split()\n",
        "    log_likelihood = calculate_log_likelihood(words_list, word_transition_matrix)\n",
        "    print(f\"Log likelihood for the sentence '{sentence}': {log_likelihood}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRdvu9aI2uXL",
        "outputId": "3f9bce10-5378-46ab-fb3b-3be4252502b5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-inf\n",
            "Log likelihood for the sentence 'This is a protest about how the new law is not in the interest of the people': -70.96470273464142\n",
            "Log likelihood for the sentence 'The international conference will continue as planned on Friday': -37.092219354590156\n",
            "Log likelihood for the sentence 'Who are you ?': -15.9902178018753\n",
            "Log likelihood for the sentence 'You are not me': -13.89888346686548\n",
            "Log likelihood for the sentence 'Do you expect to be happy to work late': -35.11892547319604\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-f721e2823648>:16: RuntimeWarning: invalid value encountered in true_divide\n",
            "  word_transition_matrix = count_words_to_next_words / np.sum(count_words_to_next_words, axis=1, keepdims=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edf1YAFvwgXV"
      },
      "source": [
        "#### Now we will continue to constructing the HMM.\n",
        "\n",
        "We will use the hmmlearn implementation to initialize the HMM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IjanyLMiwzPa"
      },
      "outputs": [],
      "source": [
        "model = hmm.MultinomialHMM(n_components=len(tags), algorithm='viterbi', random_state=42)\n",
        "model.startprob_ = startprob\n",
        "model.transmat_ = transmat\n",
        "model.emissionprob_ = emissionprob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYDvk4HEw_SO"
      },
      "source": [
        "# Our solution\n",
        "\n",
        "To use the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later. Here is how we did this (see the code underneath): \n",
        "\n",
        "We first process the test data by replacing any word not present in the training data with 'UNKNOWN'. This is done to ensure that the HMM model can handle words it has not seen before. We create a list of words in the test data called word_test.\n",
        "\n",
        "Next, we create an empty list called samples, which will store the word IDs for the words in the test data. We iterate through the word_test list and append the word ID for each word to the samples list. This allows us to feed the test data to our HMM model in a numerical format.\n",
        "\n",
        "Then, we want to calculate the lengths of sentences in the test data, as this information is needed when decoding the HMM model's predictions. We can use pandas to achieve this by calling the groupby method on data_test to group the data by sentence and then calculating the size of each group using the size() method. The resulting list of sentence lengths is stored in the lengths variable.\n",
        "\n",
        "Alternatively, we can calculate the sentence lengths without using pandas. We initialize an empty list called lengths and a variable count set to 0. We iterate through the list of sentences in the test data and increment the count if the current sentence is the same as the previous one. If the sentences are different, we append the current count to the lengths list and reset the count to 1. This approach gives us the same sentence length information as the pandas solution.\n",
        "\n",
        "These steps are crucial for preparing the test data to be used with our HMM model. By replacing unknown words and converting words to their corresponding IDs, we ensure that our model can process the test data. Additionally, calculating sentence lengths allows us to accurately decode the model's predictions and evaluate its performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
        "word_test = list(data_test.Word)\n",
        "samples = []\n",
        "for i, val in enumerate(word_test):\n",
        "    samples.append([word2id[val]])\n",
        "\n",
        "# TODO:\n",
        "#Approach 1\n",
        "# Calculate the lengths of sentences in the test data using pandas\n",
        "lengths = data_test.groupby('sentence').size().tolist()\n",
        "\n",
        "#Approach 2\n",
        "lengths = []\n",
        "count = 0\n",
        "sentences = list(data_test.sentence)\n",
        "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
        "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
        "        count += 1\n",
        "    elif i > 0:\n",
        "        lengths.append(count)\n",
        "        count = 1\n",
        "    else:\n",
        "        count = 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxDDq-YaTPVh",
        "outputId": "126ec668-9d4d-4e1a-ddcd-17b5ac49420a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 345639/345639 [00:00<00:00, 357111.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3UGvRN9x2fn"
      },
      "source": [
        "Now that we have the HMM ready lets predict the best path from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5PGjZaXx6xS",
        "outputId": "2e2056af-a931-4205-a643-4d41df536abd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([28, 38, 28, ..., 17, 19, 18], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "pos_predict = model.predict(samples, lengths)\n",
        "pos_predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CZUVE2n1BVI"
      },
      "source": [
        "The hmmlearn predict function will give the best probable path for the given sentence using the Viterbi algorithm.\n",
        "\n",
        "## Task 2: Using the model parameters (startprob_, transmat_, emissionprob_) write the viterbi algorithm from scratch to calculate the best probable path and compare it with the hmmlearn implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G279w_RtjJ-"
      },
      "source": [
        "Now before using these matrices "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUzYlQjHqcXH"
      },
      "outputs": [],
      "source": [
        "def Viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array():\n",
        "    \"\"\"\n",
        "    Write the viterbi algorithm from scratch to find the best probable path\n",
        "    attr:\n",
        "      pi: initial probabilities\n",
        "      a: transition probabilities\n",
        "      b: emission probabilities\n",
        "      obs: list of observations\n",
        "    return:\n",
        "      array of the indices of the best hidden states\n",
        "    \"\"\"\n",
        "    # Write your function here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnHjHcLJ4Kz1"
      },
      "source": [
        "### Task 3: Let's try to form our own HMM\n",
        "In this task you will try to formulate your own HMM. Image a toy example that you think that closely relates to a Hidden Markov Model.\n",
        "\n",
        "Steps:\n",
        " 1. Define your hidden states\n",
        " 2. Define your observable states\n",
        " 3. Randomly generate your observations\n",
        "\n",
        "Below is an example to demonstrate:\n",
        "\n",
        "-In this toy HMM example, we have two hidden states 'healthy' and 'sick' these states relate to the state of a pet. In this example we cannot exactly know the situation of the pet if it is 'healthy' or 'sick'\n",
        "\n",
        "-The observable states in this formulation is the what our pet is doing, whether it is sleeping, eating or pooping. We ideally want to determine if the pet is sick or not using these observable states\n",
        "\n",
        "\n",
        "```python\n",
        "hidden_states = ['healthy', 'sick']\n",
        "observable_states = ['sleeping', 'eating', 'pooping']\n",
        "observations = []\n",
        "for i in range(100):\n",
        "  observations.append(random.choice(observable_states))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2WAl5Pw7Oud"
      },
      "source": [
        "# TASK 3\n",
        "\n",
        "This is how we created our own HMM from scratch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code cell below, we first define our hidden states and observable states. The hidden states correspond to the POS tags, while the observable states represent the unique words in the dataset. These states serve as the foundation for constructing our HMM.\n",
        "\n",
        "Next, we initialize the probability matrices for our HMM. We use the startprob for initial state probabilities, the transmat for transition probabilities, and the emissionprob for emission probabilities. These matrices are derived from the training data and are essential for modeling the relationship between the hidden states and observable states.\n",
        "\n",
        "After setting up the HMM's probability matrices, we define a function called generate_observation_sequence that takes the number of observations as input and returns a sequence of words generated by the HMM. This function simulates the process of generating text based on the underlying HMM model.\n",
        "\n",
        "The function starts by randomly choosing an initial hidden state (POS tag) based on the init_probs distribution. It then selects an observable state (word) based on the emission probabilities for the chosen hidden state. This pair of hidden and observable states represents the starting point for our observation sequence.\n",
        "\n",
        "We then iterate through the remaining number of observations, selecting the next hidden state based on the transition probabilities from the current hidden state. Subsequently, we choose the next observable state (word) based on the emission probabilities for the selected hidden state. We continue this process until the desired number of observations is reached, creating a sequence of words generated by the HMM.\n",
        "\n",
        "Finally, we demonstrate the usage of the generate_observation_sequence function by generating a sequence of 10 words. The generated sequence serves as an example of text that can be produced by our HMM based on the probability matrices derived from the training data.\n",
        "\n",
        "These steps are important for building and simulating an HMM from scratch. By defining the hidden and observable states and initializing the probability matrices, we can create an HMM that models the relationships between POS tags and words in our dataset. The generate_observation_sequence function allows us to observe how the HMM generates text based on these relationships, which can be useful for understanding the underlying structure of the language and generating new sentences."
      ],
      "metadata": {
        "id": "xMJkAJrZBFSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Define hidden states and observable states\n",
        "hidden_states = tags  # POS tags\n",
        "observable_states = words  # Unique words in the dataset\n",
        "\n",
        "# Step 2: Initialize probability matrices\n",
        "num_hidden_states = len(hidden_states)\n",
        "num_observable_states = len(observable_states)\n",
        "\n",
        "# Initial state probabilities\n",
        "init_probs = startprob\n",
        "\n",
        "# Transition probabilities\n",
        "trans_probs = transmat\n",
        "\n",
        "# Emission probabilities\n",
        "emission_probs = emissionprob\n",
        "\n",
        "# Step 3: Generate a sequence of observations\n",
        "def generate_observation_sequence(num_observations: int) -> List[str]:\n",
        "    hidden_state_sequence = [np.random.choice(hidden_states, p=init_probs)]\n",
        "    observation_sequence = [np.random.choice(observable_states, p=emission_probs[tag2id[hidden_state_sequence[-1]]])]\n",
        "\n",
        "    for _ in range(1, num_observations):\n",
        "        next_hidden_state = np.random.choice(hidden_states, p=trans_probs[tag2id[hidden_state_sequence[-1]]])\n",
        "        next_observation = np.random.choice(observable_states, p=emission_probs[tag2id[next_hidden_state]])\n",
        "        \n",
        "        hidden_state_sequence.append(next_hidden_state)\n",
        "        observation_sequence.append(next_observation)\n",
        "    \n",
        "    return observation_sequence\n",
        "\n",
        "# Example usage\n",
        "num_observations = 10\n",
        "observation_sequence = generate_observation_sequence(num_observations)\n",
        "print(observation_sequence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHhND1l_d086",
        "outputId": "9a1f4e72-b2a2-475f-ba0a-32c0adb139ea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['workers', 'are', 'in', 'the', 'reporter', 'men', '.', '\"', 'europe', ',']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWCJyGjS7hKp"
      },
      "source": [
        "Even tough we have generated the data randomly, for the learning purposes, let's try to learn an HMM from this data. For this we have to construct the Baum-Welch algorithm from scratch. Below is the skeleton of the Baum-Welch learning algorithm.\n",
        "\n",
        "## TASK 4: Complete the forward and backward probs functions in the Baum-Welch algorithm and try it with your formulated HMM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6mJpFUg7K2V",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def baum_welch(observations, observations_vocab, n_hidden_states):\n",
        "    \"\"\"\n",
        "    Baum-Welch algorithm for estimating the HMM parameters\n",
        "    :param observations: observations\n",
        "    :param observations_vocab: observations vocabulary\n",
        "    :param n_hidden_states: number of hidden states to estimate\n",
        "    :return: a, b (transition matrix and emission matrix)\n",
        "    \"\"\"\n",
        "\n",
        "    def forward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
        "        \"\"\"\n",
        "        forward pass to calculate alpha\n",
        "        :param observations: observations\n",
        "        :param observations_vocab: observation vocabulary\n",
        "        :param n_hidden_states: number of hidden states\n",
        "        :param a_: estimated alpha\n",
        "        :param b_: estimated beta\n",
        "        :return: refined alpha_\n",
        "        \"\"\"\n",
        "        a_start = 1 / n_hidden_states\n",
        "        alpha_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
        "        #TODO complete the forward function to calculate alpha\n",
        "\n",
        "        return alpha_\n",
        "\n",
        "    def backward_probs(observations, observations_vocab, n_hidden_states, a_, b_) -> np.array:\n",
        "        \"\"\"\n",
        "        backward pass to calculate alpha\n",
        "        :param observations: observations\n",
        "        :param observations_vocab: observation vocabulary\n",
        "        :param n_hidden_states: number of hidden states\n",
        "        :param a_: estimated alpha\n",
        "        :param b_: estimated beta\n",
        "        :return: refined beta_\n",
        "        \"\"\"\n",
        "        beta_ = np.zeros((n_hidden_states, len(observations)), dtype=float)\n",
        "        beta_[:, -1:] = 1\n",
        "        # TODO finish the function to calculate backward pass and calculate beta\n",
        "        return beta_\n",
        "\n",
        "    def compute_gamma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
        "        \"\"\"\n",
        "\n",
        "        :param alfa:\n",
        "        :param beta:\n",
        "        :param observations:\n",
        "        :param vocab:\n",
        "        :param n_samples:\n",
        "        :param a_:\n",
        "        :param b_:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        # gamma_prob = np.zeros(n_samples, len(observations))\n",
        "        gamma_prob = np.multiply(alfa, beta) / sum(np.multiply(alfa, beta))\n",
        "        return gamma_prob\n",
        "\n",
        "    def compute_sigma(alfa, beta, observations, vocab, n_samples, a_, b_) -> np.array:\n",
        "        \"\"\"\n",
        "\n",
        "        :param alfa:\n",
        "        :param beta:\n",
        "        :param observations:\n",
        "        :param vocab:\n",
        "        :param n_samples:\n",
        "        :param a_:\n",
        "        :param b_:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        sigma_prob = np.zeros((n_samples, len(observations) - 1, n_samples), dtype=float)\n",
        "        denomenator = np.multiply(alfa, beta)\n",
        "        for i in range(len(observations) - 1):\n",
        "            for j in range(n_samples):\n",
        "                for k in range(n_samples):\n",
        "                    index_in_vocab = np.where(vocab == observations[i + 1])[0][0]\n",
        "                    sigma_prob[j, i, k] = (alfa[j, i] * beta[k, i + 1] * a_[j, k] * b_[k, index_in_vocab]) / sum(\n",
        "                        denomenator[:, j])\n",
        "        return sigma_prob\n",
        "\n",
        "    # initialize A ,B\n",
        "    a = np.ones((n_hidden_states, n_hidden_states)) / n_hidden_states\n",
        "    b = np.ones((n_hidden_states, len(observations_vocab))) / len(observations_vocab)\n",
        "    for iter in tqdm(range(2000), position=0, leave=True):\n",
        "\n",
        "        # E-step caclculating sigma and gamma\n",
        "        alfa_prob = forward_probs(observations, observations_vocab, n_hidden_states, a, b)  #\n",
        "        beta_prob = backward_probs(observations, observations_vocab, n_hidden_states, a, b)  # , beta_val\n",
        "        gamma_prob = compute_gamma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
        "        sigma_prob = compute_sigma(alfa_prob, beta_prob, observations, observations_vocab, n_hidden_states, a, b)\n",
        "\n",
        "        # M-step caclculating A, B matrices\n",
        "        a_model = np.zeros((n_hidden_states, n_hidden_states))\n",
        "        for j in range(n_hidden_states):  # calculate A-model\n",
        "            for i in range(n_hidden_states):\n",
        "                for t in range(len(observations) - 1):\n",
        "                    a_model[j, i] = a_model[j, i] + sigma_prob[j, t, i]\n",
        "                normalize_a = [sigma_prob[j, t_current, i_current] for t_current in range(len(observations) - 1) for\n",
        "                               i_current in range(n_hidden_states)]\n",
        "                normalize_a = sum(normalize_a)\n",
        "                if normalize_a == 0:\n",
        "                    a_model[j, i] = 0\n",
        "                else:\n",
        "                    a_model[j, i] = a_model[j, i] / normalize_a\n",
        "\n",
        "        b_model = np.zeros((n_hidden_states, len(observations_vocab)))\n",
        "\n",
        "        for j in range(n_hidden_states):\n",
        "            for i in range(len(observations_vocab)):\n",
        "                indices = [idx for idx, val in enumerate(observations) if val == observations_vocab[i]]\n",
        "                numerator_b = sum(gamma_prob[j, indices])\n",
        "                denominator_b = sum(gamma_prob[j, :])\n",
        "                if denominator_b == 0:\n",
        "                    b_model[j, i] = 0\n",
        "                else:\n",
        "                    b_model[j, i] = numerator_b / denominator_b\n",
        "\n",
        "        a = a_model\n",
        "        b = b_model\n",
        "    return a, b\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "hidden_states = ['healthy', 'sick']\n",
        "observable_states = ['sleeping', 'eating', 'pooping']\n",
        "observable_map = {'sleeping': 0, 'eating': 1, 'pooping': 2}\n",
        "observations = []\n",
        "for i in range(100):\n",
        "    observations.append(observable_map[random.choice(observable_states)])\n",
        "\n",
        "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
        "                  n_hidden_states=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJzrpdwJEf0c"
      },
      "outputs": [],
      "source": [
        "#TASK 4: Now try it with your HMM"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}